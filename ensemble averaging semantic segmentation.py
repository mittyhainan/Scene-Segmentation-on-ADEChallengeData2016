# -*- coding: utf-8 -*-
"""6.869 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11krTXInZpNFgA0Wz5NNPcviPHG8TGcf7
"""

import os
from tqdm import tqdm
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import torch
import random
from PIL import Image, ImageOps, ImageFilter
from torchvision import transforms
from torch import nn
import torch.nn.functional as F
from torch.utils import data
import time
import copy
from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    print("Using the GPU!")
else:
    print("WARNING: Could not find GPU! Using CPU only")
    print("You may want to try to use the GPU in Google Colab by clicking in:")
    print("Runtime > Change Runtime type > Hardware accelerator > GPU.")

"""Downloading the pacakges of U-net"""

!pip install git+https://github.com/qubvel/segmentation_models.pytorch
import segmentation_models_pytorch as smp

"""import dataset from mit"""

#!wget http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip
#!unzip ADEChallengeData2016.zip && rm ADEChallengeData2016.zip
#!mv /content/ADEChallengeData2016 /content/drive/MyDrive/6.869

#!unzip ADEChallengeData2016.zip
#!mv /content/ADEChallengeData2016 /content/drive/MyDrive/6.869

TARGET_GOOGLE_DRIVE_DIR = '/content/drive/MyDrive/6.869/ADEChallengeData2016'

"""Show some images

#Take a Look on Dataset
"""

image_name_list = ["ADE_train_00000002","ADE_train_00000003","ADE_train_00000005"]
fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)
for i,image_name in enumerate(image_name_list):
  original_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
  label_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR, "annotations", "training", f"{image_name}.png")
  original_img = Image.open(original_image).convert('RGB')
  mask_img = Image.open(label_image)

  axs[i,0].imshow(original_img)
  axs[i,0].grid(False)

  label_image = np.asarray(mask_img)
  axs[i,1].imshow(label_image)
  axs[i,1].grid(False)

class SegmentationDataset(object):
    """Segmentation Base Dataset"""

    def __init__(self, root, split, mode, transform, base_size=520, crop_size=480):
        super(SegmentationDataset, self).__init__()
        self.root = root
        self.transform = transform
        self.split = split
        self.mode = mode if mode is not None else split
        self.base_size = base_size
        self.crop_size = crop_size

    def _val_sync_transform(self, img, mask):
        outsize = self.crop_size
        short_size = outsize
        w, h = img.size
        if w > h:
            oh = short_size
            ow = int(1.0 * w * oh / h)
        else:
            ow = short_size
            oh = int(1.0 * h * ow / w)
        img = img.resize((ow, oh), Image.BILINEAR)
        mask = mask.resize((ow, oh), Image.NEAREST)
        # center crop
        w, h = img.size
        x1 = int(round((w - outsize) / 2.))
        y1 = int(round((h - outsize) / 2.))
        img = img.crop((x1, y1, x1 + outsize, y1 + outsize))
        mask = mask.crop((x1, y1, x1 + outsize, y1 + outsize))
        # final transform
        img, mask = self._img_transform(img), self._mask_transform(mask)
        return img, mask

    def _sync_transform(self, img, mask):
        # random mirror
        if random.random() < 0.5:
            img = img.transpose(Image.FLIP_LEFT_RIGHT)
            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)
        crop_size = self.crop_size
        # random scale (short edge)
        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))
        w, h = img.size
        if h > w:
            ow = short_size
            oh = int(1.0 * h * ow / w)
        else:
            oh = short_size
            ow = int(1.0 * w * oh / h)
        img = img.resize((ow, oh), Image.BILINEAR)
        mask = mask.resize((ow, oh), Image.NEAREST)
        # pad crop
        if short_size < crop_size:
            padh = crop_size - oh if oh < crop_size else 0
            padw = crop_size - ow if ow < crop_size else 0
            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)
            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=0)
        # random crop crop_size
        w, h = img.size
        x1 = random.randint(0, w - crop_size)
        y1 = random.randint(0, h - crop_size)
        img = img.crop((x1, y1, x1 + crop_size, y1 + crop_size))
        mask = mask.crop((x1, y1, x1 + crop_size, y1 + crop_size))
        # gaussian blur as in PSP
        if random.random() < 0.5:
            img = img.filter(ImageFilter.GaussianBlur(radius=random.random()))
        # final transform
        img, mask = self._img_transform(img), self._mask_transform(mask)
        return img, mask

    def _img_transform(self, img):
        return np.array(img)

    def _mask_transform(self, mask):
        return np.array(mask).astype('int32')

    @property
    def num_class(self):
        """Number of categories."""
        return self.NUM_CLASS

    @property
    def pred_offset(self):
        return 0

class ADE20KSegmentation(SegmentationDataset):
    """ADE20K Semantic Segmentation Dataset.
    Parameters
    ----------
    root : string
        Path to ADE20K folder. Default is './datasets/ade'
    split: string
        'train', 'val' or 'test'
    transform : callable, optional
        A function that transforms the image
    Examples
    --------
    >>> from torchvision import transforms
    >>> import torch.utils.data as data
    >>> # Transforms for Normalization
    >>> input_transform = transforms.Compose([
    >>>     transforms.ToTensor(),
    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),
    >>> ])
    >>> # Create Dataset
    >>> trainset = ADE20KSegmentation(split='train', transform=input_transform)
    >>> # Create Training Loader
    >>> train_data = data.DataLoader(
    >>>     trainset, 4, shuffle=True,
    >>>     num_workers=4)
    """
    BASE_DIR = 'ADEChallengeData2016'
    NUM_CLASS = 150

    def __init__(self, root='/content/drive/MyDrive/6.869', split='train', mode=None, transform=None, **kwargs):
        super(ADE20KSegmentation, self).__init__(root, split, mode, transform, **kwargs)
        root = os.path.join(root, self.BASE_DIR)
        self.images, self.masks = _get_ade20k_pairs(root, split)
        assert (len(self.images) == len(self.masks))
        if len(self.images) == 0:
            raise RuntimeError("Found 0 images in subfolders of:" + root + "\n")
        print('Found {} images in the folder {}'.format(len(self.images), root))

    def __getitem__(self, index):
        img = Image.open(self.images[index]).convert('RGB')
        if self.mode == 'test':
            img = self._img_transform(img)
            if self.transform is not None:
                img = self.transform(img)
            return img, os.path.basename(self.images[index])
        mask = Image.open(self.masks[index])
        # synchrosized transform
        if self.mode == 'train':
            img, mask = self._sync_transform(img, mask)
        elif self.mode == 'val':
            img, mask = self._val_sync_transform(img, mask)
        else:
            assert self.mode == 'testval'
            img, mask = self._img_transform(img), self._mask_transform(mask)
        # general resize, normalize and to Tensor
        if self.transform is not None:
            img = self.transform(img)
        return img, mask, os.path.basename(self.images[index])

    def _mask_transform(self, mask):
        return torch.LongTensor(np.array(mask).astype('int32') - 1)

    def __len__(self):
        return len(self.images)

    @property
    def pred_offset(self):
        return 1

    @property
    def classes(self):
        """Category names."""
        return ("wall", "building, edifice", "sky", "floor, flooring", "tree",
                "ceiling", "road, route", "bed", "windowpane, window", "grass",
                "cabinet", "sidewalk, pavement",
                "person, individual, someone, somebody, mortal, soul",
                "earth, ground", "door, double door", "table", "mountain, mount",
                "plant, flora, plant life", "curtain, drape, drapery, mantle, pall",
                "chair", "car, auto, automobile, machine, motorcar",
                "water", "painting, picture", "sofa, couch, lounge", "shelf",
                "house", "sea", "mirror", "rug, carpet, carpeting", "field", "armchair",
                "seat", "fence, fencing", "desk", "rock, stone", "wardrobe, closet, press",
                "lamp", "bathtub, bathing tub, bath, tub", "railing, rail", "cushion",
                "base, pedestal, stand", "box", "column, pillar", "signboard, sign",
                "chest of drawers, chest, bureau, dresser", "counter", "sand", "sink",
                "skyscraper", "fireplace, hearth, open fireplace", "refrigerator, icebox",
                "grandstand, covered stand", "path", "stairs, steps", "runway",
                "case, display case, showcase, vitrine",
                "pool table, billiard table, snooker table", "pillow",
                "screen door, screen", "stairway, staircase", "river", "bridge, span",
                "bookcase", "blind, screen", "coffee table, cocktail table",
                "toilet, can, commode, crapper, pot, potty, stool, throne",
                "flower", "book", "hill", "bench", "countertop",
                "stove, kitchen stove, range, kitchen range, cooking stove",
                "palm, palm tree", "kitchen island",
                "computer, computing machine, computing device, data processor, "
                "electronic computer, information processing system",
                "swivel chair", "boat", "bar", "arcade machine",
                "hovel, hut, hutch, shack, shanty",
                "bus, autobus, coach, charabanc, double-decker, jitney, motorbus, "
                "motorcoach, omnibus, passenger vehicle",
                "towel", "light, light source", "truck, motortruck", "tower",
                "chandelier, pendant, pendent", "awning, sunshade, sunblind",
                "streetlight, street lamp", "booth, cubicle, stall, kiosk",
                "television receiver, television, television set, tv, tv set, idiot "
                "box, boob tube, telly, goggle box",
                "airplane, aeroplane, plane", "dirt track",
                "apparel, wearing apparel, dress, clothes",
                "pole", "land, ground, soil",
                "bannister, banister, balustrade, balusters, handrail",
                "escalator, moving staircase, moving stairway",
                "ottoman, pouf, pouffe, puff, hassock",
                "bottle", "buffet, counter, sideboard",
                "poster, posting, placard, notice, bill, card",
                "stage", "van", "ship", "fountain",
                "conveyer belt, conveyor belt, conveyer, conveyor, transporter",
                "canopy", "washer, automatic washer, washing machine",
                "plaything, toy", "swimming pool, swimming bath, natatorium",
                "stool", "barrel, cask", "basket, handbasket", "waterfall, falls",
                "tent, collapsible shelter", "bag", "minibike, motorbike", "cradle",
                "oven", "ball", "food, solid food", "step, stair", "tank, storage tank",
                "trade name, brand name, brand, marque", "microwave, microwave oven",
                "pot, flowerpot", "animal, animate being, beast, brute, creature, fauna",
                "bicycle, bike, wheel, cycle", "lake",
                "dishwasher, dish washer, dishwashing machine",
                "screen, silver screen, projection screen",
                "blanket, cover", "sculpture", "hood, exhaust hood", "sconce", "vase",
                "traffic light, traffic signal, stoplight", "tray",
                "ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, "
                "dustbin, trash barrel, trash bin",
                "fan", "pier, wharf, wharfage, dock", "crt screen",
                "plate", "monitor, monitoring device", "bulletin board, notice board",
                "shower", "radiator", "glass, drinking glass", "clock", "flag")


def _get_ade20k_pairs(folder, mode='train'):
    img_paths = []
    mask_paths = []
    if mode == 'train':
        img_folder = os.path.join(folder, 'images/training')
        mask_folder = os.path.join(folder, 'annotations/training')
    else:
        img_folder = os.path.join(folder, 'images/validation')
        mask_folder = os.path.join(folder, 'annotations/validation')
    for filename in os.listdir(img_folder):
        basename, _ = os.path.splitext(filename)
        if filename.endswith(".jpg"):
            imgpath = os.path.join(img_folder, filename)
            maskname = basename + '.png'
            maskpath = os.path.join(mask_folder, maskname)
            if os.path.isfile(maskpath):
                img_paths.append(imgpath)
                mask_paths.append(maskpath)
            else:
                print('cannot find the mask:', maskpath)

    return img_paths, mask_paths


if __name__ == '__main__':
    train_dataset = ADE20KSegmentation()

"""Fully Convolutional Networks"""

FCN_model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)

image_name_list = ["ADE_train_00000002","ADE_train_00000003","ADE_train_00000005"]

preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

FCN_model.to('cuda')
fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)

for i,image_name in enumerate(image_name_list):
  input_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
  input_image = Image.open(input_image).convert('RGB')

  input_tensor = preprocess(input_image)
  #add batch dimension
  input_batch = input_tensor.unsqueeze(0)


  input_batch = input_batch.to('cuda')

  with torch.no_grad():
    output = FCN_model(input_batch)['out'][0]
  output_predictions = output.argmax(0)

  palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
  colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
  colors = (colors % 255).numpy().astype("uint8")

# plot the semantic segmentation predictions of 21 classes in each color
  r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
  r.putpalette(colors)
  axs[i,0].imshow(input_image)
  axs[i,1].imshow(r)

new_FCN_model = FCN_model
new_FCN_model.classifier[4] = nn.Conv2d(512, 150, kernel_size=(1, 1), stride=(1, 1))
print(new_FCN_model.classifier)

new_FCN_model.aux_classifier[4] = nn.Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))
print(new_FCN_model.classifier)

"""evaluate the initial new fcn no train, better than deeplabv3"""

new_FCN_model.to(device)
evaluate_model(new_FCN_model,loss_f,validation_dataloader)

image_name_list = ["ADE_train_00000002","ADE_train_00000003","ADE_train_00000005"]
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
new_FCN_model.to('cuda')
fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)

for i,image_name in enumerate(image_name_list):
  input_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
  input_image = Image.open(input_image).convert('RGB')

  input_tensor = preprocess(input_image)
  input_batch = input_tensor.unsqueeze(0)


  input_batch = input_batch.to('cuda')

  with torch.no_grad():
    output = new_FCN_model(input_batch)
    print(type(output))
    print(output['out'].shape)
    output = output['out'][0]
  output_predictions = output.argmax(0)

  palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
  colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
  colors = (colors % 255).numpy().astype("uint8")

# plot the semantic segmentation predictions of 21 classes in each color
  r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
  r.putpalette(colors)
  axs[i,0].imshow(input_image)
  axs[i,1].imshow(r)

#retrain the model
image_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    transforms.CenterCrop(480)
])

trainset = ADE20KSegmentation(split='train', transform=image_transform)
validationset = ADE20KSegmentation(split='val', transform=image_transform)
testset = ADE20KSegmentation(split='test', transform=image_transform)

# Create Training Loader
train_dataloader = data.DataLoader(
    trainset, batch_size = 4, shuffle=True,
    num_workers=2)
validation_dataloader = data.DataLoader(
    validationset,batch_size = 4, shuffle=True,
    num_workers=2
)
test_dataloader = data.DataLoader(
    testset, batch_size = 4, shuffle=True,
    num_workers = 2
)

fig, axs = plt.subplots(1, 2, figsize=(8, 4), constrained_layout=True)
train_data= next(iter(train_dataloader))
print(f"Feature batch shape: {train_data[0].size()}")
print(f"Labels batch shape: {train_data[1].size()}")
train_image = torch.tensor(train_data[0][0, :, :, :]).permute(1,2,0)
axs[0].imshow(train_image)
semantic_image = train_data[1][0,:,:]
axs[1].imshow(semantic_image)

"""#Basic Helper Functions"""

def visualize_performance_on_sample_image(model,dataset = "training",index = 0):
  if dataset == "training":
    if index == 0:
      image_name_list = ["ADE_train_00000002","ADE_train_00000003","ADE_train_00000005"]
    else:
      image_name_list = ["ADE_train_00000009","ADE_train_00000007","ADE_train_00000008"]
  else:
    if index == 0:
      image_name_list = ["ADE_val_00000002","ADE_val_00000003","ADE_val_00000005"]
    else:
      image_name_list = ["ADE_val_00000009","ADE_val_00000007","ADE_val_00000008"]
  preprocess = transforms.Compose([
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
  ])
  model.to('cuda')
  fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)

  for i,image_name in enumerate(image_name_list):
    if dataset == "training":
      input_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
    else:
      input_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","validation",f"{image_name}.jpg")
    input_image = Image.open(input_image).convert('RGB')

    input_tensor = preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)


    input_batch = input_batch.to('cuda')

    with torch.no_grad():
      output = model(input_batch)
      print(type(output))
      print(output['out'].shape)
      output = output['out'][0]
    output_predictions = output.argmax(0)

    #palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
    #colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
    #colors = (colors % 255).numpy().astype("uint8")

# plot the semantic segmentation predictions of 21 classes in each color
    r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
    #r.putpalette(colors)
    axs[i,0].imshow(input_image)
    axs[i,1].imshow(r)

def one_hot_encoding(label,label_values = range(150)):
  semantic_map = []
  for i in label_values:
    class_map = (label == i)*1
    semantic_map.append(class_map)
  mask_coded = np.stack(semantic_map,axis = -1)
  mask_coded = torch.tensor(mask_coded)
  mask_coded = mask_coded.permute(0,3,1,2)
  return mask_coded

def IoU_compute(output,label,num_classes = 150):
  output = torch.argmax(output, dim=1).squeeze(1)
  output = output.view(-1)
  label = label.view(-1)

  iou_list = []
  present_iou_list = []

  for semantic_class in range(num_classes):
    output_inds = (output == semantic_class)
    label_inds = (label == semantic_class)
    if label_inds.long().sum().item() == 0:
      iou_now = float('nan')
    else:
      intersection_now = (output_inds[label_inds]).long().sum().item()
      union_now = label_inds.long().sum().item() + output_inds.long().sum().item() - intersection_now
      iou_now = float(intersection_now) / float(union_now)
      present_iou_list.append(iou_now)
    iou_list.append(iou_now)
    if len(present_iou_list) == 0:
      avg_iou = 0
    else:
      avg_iou = np.mean(present_iou_list)*100
  return avg_iou

def class_IoU_compute(output,label,num_classes = 150):
  output = torch.argmax(output, dim=1).squeeze(1)
  output = output.view(-1)
  label = label.view(-1)

  class_iou_list = []

  for semantic_class in range(num_classes):
    output_inds = (output == semantic_class)
    label_inds = (label == semantic_class)
    if label_inds.long().sum().item() == 0:
      iou_now = float('nan')
    else:
      intersection_now = (output_inds[label_inds]).long().sum().item()
      union_now = label_inds.long().sum().item() + output_inds.long().sum().item() - intersection_now
      iou_now = float(intersection_now) / float(union_now)
    class_iou_list.append(iou_now)
  return class_iou_list

def compute_classification_accuracy(output,label):
  output_predictions = output.argmax(1)
  output_predictions = output.argmax(1)
  output_predictions = output_predictions.reshape(-1,1)
  output_predictions = output_predictions.squeeze(1)
  label = label.reshape(-1,1)
  label = label.squeeze(1)
  mean = (label == output_predictions).sum()/len(output_predictions)
  return mean

optimizer = torch.optim.SGD(new_FCN_model.parameters(), lr=0.001, momentum=0.9)
loss_f = nn.CrossEntropyLoss()
def train_model(model,train_dataloader,validation_dataloader,optimizer,loss_f,num_epochs):
  since = time.time()
  train_loss = []
  running_loss = 0.0

  for epoch in range(num_epochs):
    print('Epoch {}/{}'.format(epoch + 1, num_epochs))
    print('-' * 10)
    train_loss_per_100_itr = 0
    training_mean_iou_total = 0
    training_mean_iou_per_100_itr = 0

    for i in tqdm(range(2000)):
    #for i,input_pairs in enumerate(train_dataloader):
      input_pairs = next(iter(train_dataloader))
      train_data = input_pairs[0]
      semantic_label = input_pairs[1]
      train_data = train_data.to(device)
      semantic_data = one_hot_encoding(semantic_label)
      semantic_label = semantic_label.to(device)
      semantic_data = semantic_data.to(device)

      optimizer.zero_grad()
      output = model(train_data)['out']

      iou_per_batch = IoU_compute(output,semantic_label)
      training_mean_iou_total += iou_per_batch
      training_mean_iou_per_100_itr += iou_per_batch

      loss = loss_f(output,semantic_data.float())
      loss.backward()
      optimizer.step()

      count = i + 1
      if count % 100 == 0:
        print("avg train loss per 100 itr: ", train_loss_per_100_itr/100)
        print("avg train mean iou per 100 itr: ", training_mean_iou_per_100_itr/100)
        training_mean_iou_per_100_itr = 0
        train_loss_per_100_itr = 0
      elif i == 0:
        print("initial loss: ", loss.item())
      else:
        train_loss_per_100_itr += loss.item()
      

      running_loss += loss.item()

    epoch_loss = running_loss/2000
    mean_iou_per_epoch = training_mean_iou_per_100_itr/2000
    train_loss.append(epoch_loss)
    print("train loss per epoch: ", train_loss)
    print("train loss per epoch: ", mean_iou_per_epoch)

  return model,train_loss

def evaluate_model(model,loss_f,test_dataloader):
  total_test_loss = 0
  test_loss_itr = 0
  validation_mean_iou = 0
  validation_mean_accuracy = 0
  mean_iou_per_100 = 0
  mean_accuracy_per_100 = 0
  for i in tqdm(range(500)):
    test_pair = next(iter(test_dataloader))
    test_data = test_pair[0]
    semantic_label = test_pair[1]
    semantic_data = one_hot_encoding(semantic_label).float()
    test_data = test_data.to(device)
    semantic_data = semantic_data.to(device)
    semantic_label = semantic_label.to(device)
    count = i+1

    with torch.no_grad():
      output = model(test_data)['out']
      iou_per_batch = IoU_compute(output,semantic_label)
      accuracy_per_batch = compute_classification_accuracy(output,semantic_label)
      validation_mean_accuracy += accuracy_per_batch.item()
      validation_mean_iou += iou_per_batch
      mean_accuracy_per_100 += accuracy_per_batch.item()
      mean_iou_per_100 += iou_per_batch
      test_loss = loss_f(output,semantic_data)
      total_test_loss += test_loss.item()
      test_loss_itr += test_loss.item()

      if i == 0:
        print("initial validation loss: ", test_loss.item())
        print("======")
        print("initial validation IOU: ", iou_per_batch)
        print("======")
        print("initial validation accuracy ", accuracy_per_batch.item())
      elif count % 100 == 0:
        print("validation loss per 100 itr", test_loss_itr/100)
        print("======")
        print("validation mean IOU per 100 itr: ", mean_iou_per_100/100)
        print("======")
        print("validation mean accuracy per 100 iter ", mean_accuracy_per_100/100)
        mean_iou_per_100 = 0
        test_loss_itr = 0
        mean_accuracy_per_100 = 0

  avg_test_loss = total_test_loss/500
  avg_iou = validation_mean_iou/500
  avg_accuracy = validation_mean_accuracy/500
  print("final mean IOU: ", avg_iou)
  print("test loss: ", avg_test_loss)
  print("test accuracy: ", avg_accuracy)
  return avg_test_loss,avg_iou,avg_accuracy

"""#FCN

After training 2000 images
"""

image_name_list = ["ADE_train_00000002","ADE_train_00000003","ADE_train_00000005"]
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
new_FCN_model.to('cuda')
fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)

for i,image_name in enumerate(image_name_list):
  input_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
  input_image = Image.open(input_image).convert('RGB')

  input_tensor = preprocess(input_image)
  input_batch = input_tensor.unsqueeze(0)


  input_batch = input_batch.to('cuda')

  with torch.no_grad():
    output = new_FCN_model(input_batch)
    print(type(output))
    print(output['out'].shape)
    output = output['out'][0]
  output_predictions = output.argmax(0)

  palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])
  colors = torch.as_tensor([i for i in range(21)])[:, None] * palette
  colors = (colors % 255).numpy().astype("uint8")

# plot the semantic segmentation predictions of 21 classes in each color
  r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
  r.putpalette(colors)
  axs[i,0].imshow(input_image)
  axs[i,1].imshow(r)

"""save current trained model"""

model_name = "new_FCN"
path = '/content/drive/MyDrive/6.869/{model_name}'
torch.save(new_FCN_model.state_dict(), path)

"""train another epoch"""

model_name = "new_FCN"
path = '/content/drive/MyDrive/6.869/{model_name}'
new_FCN_model.load_state_dict(torch.load(path))

train_dataloader = data.DataLoader(
    trainset, batch_size = 4, shuffle=True,
    num_workers=2)
validation_dataloader = data.DataLoader(
    validationset,batch_size = 4, shuffle=True,
    num_workers=2
)

new_FCN_model.to(device)

#second time
#train another epoch
train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,1)

visualize_performance_on_sample_image()

model_name = "new_FCN2"
path = '/content/drive/MyDrive/6.869/{model_name}'
torch.save(new_FCN_model.state_dict(), path)

"""detection of wall, floor is getting better but human detection performance seems decay"""

#third time
train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,1)

visualize_performance_on_sample_image()

model_name = "new_FCN3"
path = '/content/drive/MyDrive/6.869/{model_name}'
torch.save(new_FCN_model.state_dict(), path)

model_name = "new_FCN3"
path = '/content/drive/MyDrive/6.869/{model_name}'
new_FCN_model.load_state_dict(torch.load(path))

new_FCN_model.to(device)
train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,1)

visualize_performance_on_sample_image()

model_name = "new_FCN4"
path = '/content/drive/MyDrive/6.869/{model_name}'
torch.save(new_FCN_model.state_dict(), path)

model_name = "new_FCN4"
path = '/content/drive/MyDrive/6.869/{model_name}'
new_FCN_model.load_state_dict(torch.load(path))

new_FCN_model.to(device)
train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,2)
model_name_4 = "new_FCN5"
path = '/content/drive/MyDrive/6.869/{model_name_4}'
torch.save(new_FCN_model.state_dict(), path)
visualize_performance_on_sample_image()

model_name_4 = "new_FCN5"
path = '/content/drive/MyDrive/6.869/{model_name_4}'
new_FCN_model.load_state_dict(torch.load(path))
new_FCN_model.to(device)
train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,1)

visualize_performance_on_sample_image()

"""ground truth for another set of images"""

image_name_list = ["ADE_train_00000009","ADE_train_00000007","ADE_train_00000008"]
fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)
for i,image_name in enumerate(image_name_list):
  original_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
  label_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR, "annotations", "training", f"{image_name}.png")
  original_img = Image.open(original_image).convert('RGB')
  mask_img = Image.open(label_image)

  axs[i,0].imshow(original_img)
  axs[i,0].grid(False)

  label_image = np.asarray(mask_img)
  axs[i,1].imshow(label_image)
  axs[i,1].grid(False)

visualize_performance_on_sample_image()

model_name_5 = "new_FCN6"
path = '/content/drive/MyDrive/6.869/{model_name_5}'
torch.save(new_FCN_model.state_dict(), path)

train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,1)
model_name_6 = "new_FCN7"
path = '/content/drive/MyDrive/6.869/{model_name_6}'
torch.save(new_FCN_model.state_dict(), path)
visualize_performance_on_sample_image()

model_name_6 = "new_FCN7"
path = '/content/drive/MyDrive/6.869/{model_name_6}'
torch.save(new_FCN_model.state_dict(), path)

visualize_performance_on_sample_image()

model_name_6 = "new_FCN7"
path = '/content/drive/MyDrive/6.869/{model_name_6}'
new_FCN_model.load_state_dict(torch.load(path))

new_FCN_model.to(device)
train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,2)

model_name_7 = "new_FCN8"
path = '/content/drive/MyDrive/6.869/{model_name_7}'
torch.save(new_FCN_model.state_dict(), path)
visualize_performance_on_sample_image()

visualize_performance_on_sample_image(1)

model_name_7 = "new_FCN8"
path = '/content/drive/MyDrive/6.869/{model_name_7}'
new_FCN_model.load_state_dict(torch.load(path))

new_FCN_model.to(device)
train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,2)

#model_name_8 = "new_FCN9"
#path = '/content/drive/MyDrive/6.869/{model_name_8}'
#torch.save(new_FCN_model.state_dict(), path)

model_name_8 = "new_FCN9"
path = '/content/drive/MyDrive/6.869/{model_name_8}'
new_FCN_model.load_state_dict(torch.load(path))
new_FCN_model.to(device)

"""model 9"""

evaluate_model(new_FCN_model,loss_f,validation_dataloader)

"""mean iou of new_FCN_model(version 9) is 23.403"""

train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,1)

model_name_9 = "new_FCN10"
path = '/content/drive/MyDrive/6.869/{model_name_9}'
torch.save(new_FCN_model.state_dict(), path)

visualize_performance_on_sample_image(1)

visualize_performance_on_sample_image(0)

evaluate_model(new_FCN_model,loss_f,validation_dataloader)

"""evaluate on the previous model 6"""

evaluate_model(new_FCN_model,loss_f,validation_dataloader)

"""version 6 has 27.3 iou, which is the best among so-far trained model"""

model_name_6= "new_FCN7"
path = '/content/drive/MyDrive/6.869/{model_name_6}'
new_FCN_model.load_state_dict(torch.load(path))
new_FCN_model.eval()
visualize_performance_on_sample_image(new_FCN_model,"validation",0)

model_name_6 = "new_FCN_7"
path = '/content/drive/MyDrive/6.869/{model_name_6}'
new_FCN_model.load_state_dict(torch.load(path))

new_FCN_model.to(device)
visualize_performance_on_sample_image(0)

train_model(new_FCN_model,train_dataloader,validation_dataloader,optimizer,loss_f,1)
visualize_performance_on_sample_image(0)

visualize_performance_on_sample_image(0)

evaluate_model(new_FCN_model,loss_f,validation_dataloader)

"""weird thing happen like my previous model is covered and not able to recover so I retrain my model from version 6(which is different as well)"""

model_name_610 = "new_FCN610"
path = '/content/drive/MyDrive/6.869/{model_name_610}'
torch.save(new_FCN_model.state_dict(), path)

model_name_610 = "new_FCN610"
path = '/content/drive/MyDrive/6.869/{model_name_610}'
new_FCN_model.load_state_dict(torch.load(path))
new_FCN_model.to(device)
visualize_performance_on_sample_image(new_FCN_model,"validation",1)

model_name_9 = "new_FCN10"
path = '/content/drive/MyDrive/6.869/{model_name_9}'
new_FCN_model.load_state_dict(torch.load(path))
new_FCN_model.eval()
new_FCN_model.to(device)
visualize_performance_on_sample_image(new_FCN_model,"validation",1)

evaluate_model(new_FCN_model,loss_f,train_dataloader)

"""#DeepLabV3"""

deeplabv3_model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)

new_deeplabv3 = deeplabv3_model
new_deeplabv3.classifier[4] = nn.Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))
new_deeplabv3.aux_classifier[4] = nn.Conv2d(256, 150, kernel_size=(1, 1), stride=(1, 1))

new_deeplabv3.eval()
visualize_performance_on_sample_image(new_deeplabv3,"training",0)

"""evaluate the pretrained deeplabv3 model"""

new_deeplabv3.to(device)
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

"""compare pretrained very first one FCN model

initial loss is 4.58 and iou is only 0.14599. 
 lets train 1 epoachs
"""

new_deeplabv3.to(device)
optimizer_deep = torch.optim.SGD(new_deeplabv3.parameters(), lr=0.001, momentum=0.9)
train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

new_deeplabv3.eval()
visualize_performance_on_sample_image(new_deeplabv3,"training",0)

deeplabv3_1 = "new_deeplabv3_1"
path = '/content/drive/MyDrive/6.869/{deeplabv3_1}'
torch.save(new_deeplabv3.state_dict(), path)

evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

"""further training 1 epoch"""

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_2 = "new_deeplabv3_2"
path = '/content/drive/MyDrive/6.869/{deeplabv3_2}'
torch.save(new_deeplabv3.state_dict(), path)

new_deeplabv3.eval()
visualize_performance_on_sample_image(new_deeplabv3,"validation",0)

evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,2)

deeplabv3_3 = "new_deeplabv3_3"
path = '/content/drive/MyDrive/6.869/{deeplabv3_3}'
torch.save(new_deeplabv3.state_dict(), path)

new_deeplabv3.eval()
visualize_performance_on_sample_image(new_deeplabv3,"training",0)

deeplabv3_3 = "new_deeplabv3_3"
path = '/content/drive/MyDrive/6.869/{deeplabv3_3}'
new_deeplabv3.load_state_dict(torch.load(path))
new_deeplabv3.eval()
new_deeplabv3.to(device)
visualize_performance_on_sample_image(new_deeplabv3,"validation",1)

evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

new_deeplabv3.to(device)
optimizer_deep = torch.optim.SGD(new_deeplabv3.parameters(), lr=0.001, momentum=0.9)
new_deeplabv3.eval()
train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,2)

deeplabv3_4 = "new_deeplabv3_4"
path = '/content/drive/MyDrive/6.869/{deeplabv3_4}'
torch.save(new_deeplabv3.state_dict(), path)

new_deeplabv3.eval()
visualize_performance_on_sample_image(new_deeplabv3,"training",0)

evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_5 = "new_deeplabv3_5"
path = '/content/drive/MyDrive/6.869/{deeplabv3_5}'
torch.save(new_deeplabv3.state_dict(), path)

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

deeplabv3_5 = "new_deeplabv3_5"
path = '/content/drive/MyDrive/6.869/{deeplabv3_5}'
new_deeplabv3.load_state_dict(torch.load(path))
new_deeplabv3.eval()

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_6 = "new_deeplabv3_6"
path = '/content/drive/MyDrive/6.869/{deeplabv3_6}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

deeplabv3_6 = "new_deeplabv3_6"
path = '/content/drive/MyDrive/6.869/{deeplabv3_6}'
new_deeplabv3.load_state_dict(torch.load(path))
new_deeplabv3.eval()
new_deeplabv3.to(device)
optimizer_deep = torch.optim.SGD(new_deeplabv3.parameters(), lr=0.001, momentum=0.9)
train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_7 = "new_deeplabv3_7"
path = '/content/drive/MyDrive/6.869/{deeplabv3_7}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_8 = "new_deeplabv3_8"
path = '/content/drive/MyDrive/6.869/{deeplabv3_8}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

new_deeplabv3.eval()
train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,2)

deeplabv3_9 = "new_deeplabv3_9"
path = '/content/drive/MyDrive/6.869/{deeplabv3_9}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_10 = "new_deeplabv3_10"
path = '/content/drive/MyDrive/6.869/{deeplabv3_10}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_11 = "new_deeplabv3_11"
path = '/content/drive/MyDrive/6.869/{deeplabv3_11}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

deeplabv3_11 = "new_deeplabv3_11"
path = '/content/drive/MyDrive/6.869/{deeplabv3_11}'
new_deeplabv3.load_state_dict(torch.load(path))
new_deeplabv3.eval()

optimizer_deep = torch.optim.SGD(new_deeplabv3.parameters(), lr=0.001, momentum=0.9)
new_deeplabv3.to(device)
new_deeplabv3.eval()
train_model(new_deeplabv3,train_dataloader,validation_dataloader,optimizer_deep,loss_f,2)

deeplabv3_12 = "new_deeplabv3_12"
path = '/content/drive/MyDrive/6.869/{deeplabv3_12}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

deeplabv3_12 = "new_deeplabv3_12"
path = '/content/drive/MyDrive/6.869/{deeplabv3_12}'
new_deeplabv3.load_state_dict(torch.load(path))
new_deeplabv3.eval()

optimizer_deep = torch.optim.SGD(new_deeplabv3.parameters(), lr=0.001, momentum=0.9)
new_deeplabv3.to(device)
new_deeplabv3.eval()
train_model(new_deeplabv3,validation_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_13 = "new_deeplabv3_13"
path = '/content/drive/MyDrive/6.869/{deeplabv3_13}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

train_model(new_deeplabv3,validation_dataloader,validation_dataloader,optimizer_deep,loss_f,1)

deeplabv3_14 = "new_deeplabv3_14"
path = '/content/drive/MyDrive/6.869/{deeplabv3_14}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

train_model(new_deeplabv3,validation_dataloader,validation_dataloader,optimizer_deep,loss_f,2)

deeplabv3_15 = "new_deeplabv3_15"
path = '/content/drive/MyDrive/6.869/{deeplabv3_15}'
torch.save(new_deeplabv3.state_dict(), path)
new_deeplabv3.eval()
evaluate_model(new_deeplabv3,loss_f,validation_dataloader)

deeplabv3_10 = "new_deeplabv3_10"
path = '/content/drive/MyDrive/6.869/{deeplabv3_10}'
new_deeplabv3.load_state_dict(torch.load(path))

"""# Ensemble Method"""

def semantic_ensemble_visualize(model1,model2,input_batch,weight = 0.2):
  with torch.no_grad():
    output1 = model1(input_batch)
    output1 = output1['out'][0]
    output2 = model2(input_batch)
    output2 = output2['out'][0]
    output = output1*weight + output2*(1-weight)
  output_predictions = output.argmax(0)
  return output_predictions

def visualize_performance_on_sample_image_ensemble(model1,model2,dataset = "training",index = 0,weight = 0.2):
  if dataset == "training":
    if index == 0:
      image_name_list = ["ADE_train_00000002","ADE_train_00000003","ADE_train_00000005"]
    else:
      image_name_list = ["ADE_train_00000009","ADE_train_00000007","ADE_train_00000008"]
  else:
    if index == 0:
      image_name_list = ["ADE_val_00000002","ADE_val_00000003","ADE_val_00000005"]
    else:
      image_name_list = ["ADE_val_00000009","ADE_val_00000007","ADE_val_00000008"]
  preprocess = transforms.Compose([
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
  ])
  model1.to('cuda')
  model2.to('cuda')
  fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)

  for i,image_name in enumerate(image_name_list):
    if dataset == "training":
      input_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
    else:
      input_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","validation",f"{image_name}.jpg")
    input_image = Image.open(input_image).convert('RGB')

    input_tensor = preprocess(input_image)
    input_batch = input_tensor.unsqueeze(0)


    input_batch = input_batch.to('cuda')


    output_predictions = semantic_ensemble_visualize(model1,model2,input_batch,weight)

    r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)
    #r.putpalette(colors)
    axs[i,0].imshow(input_image)
    axs[i,1].imshow(r)

def semantic_ensemble_eval(model1,model2,test_data,weight):
  with torch.no_grad():
    output1 = model1(test_data)
    output1 = output1['out']
    output2 = model2(test_data)
    output2 = output2['out']
    output = output1*weight + output2*(1-weight)
  return output

def evaluate_model_ensemble(model1,model2,loss_f,test_dataloader,weight = 0.5):
  total_test_loss = 0
  test_loss_itr = 0
  validation_mean_iou = 0
  validation_mean_accuracy = 0
  mean_iou_per_100 = 0
  mean_accuracy_per_100 = 0
  for i in tqdm(range(500)):
    test_pair = next(iter(test_dataloader))
    test_data = test_pair[0]
    semantic_label = test_pair[1]
    semantic_data = one_hot_encoding(semantic_label).float()
    test_data = test_data.to(device)
    semantic_data = semantic_data.to(device)
    semantic_label = semantic_label.to(device)
    count = i+1

    with torch.no_grad():
      output = semantic_ensemble_eval(model1,model2,test_data,weight)
      iou_per_batch = IoU_compute(output,semantic_label)
      accuracy_per_batch = compute_classification_accuracy(output,semantic_label)
      validation_mean_iou += iou_per_batch
      validation_mean_accuracy += accuracy_per_batch.item()
      mean_accuracy_per_100 += accuracy_per_batch.item()
      mean_iou_per_100 += iou_per_batch
      test_loss = loss_f(output,semantic_data)
      total_test_loss += test_loss.item()
      test_loss_itr += test_loss.item()

      if i == 0:
        print("initial validation loss: ", test_loss.item())
        print("======")
        print("initial validation IOU: ", iou_per_batch)
        print("======")
        print("initial validation accuracy: ", accuracy_per_batch.item())
      elif count % 100 == 0:
        print("validation loss per 100 itr", test_loss_itr/100)
        print("======")
        print("validation mean IOU per 100 itr: ", mean_iou_per_100/100)
        print("======")
        print("validation mean accuracy per 100 ite ", mean_accuracy_per_100/100)
        mean_iou_per_100 = 0
        test_loss_itr = 0
        mean_accuracy_per_100 = 0

  avg_test_loss = total_test_loss/500
  avg_iou = validation_mean_iou/500
  avg_accuracy = validation_mean_accuracy/500
  print("final mean IOU: ", avg_iou)
  print("test loss: ", avg_test_loss)
  print("validation mean accuracy: ", avg_accuracy)
  return avg_test_loss,avg_iou,avg_accuracy

def compute_classification_accuracy(model,test_dataloader):
  test_pair = next(iter(validation_dataloader))
  test_data = test_pair[0]
  semantic_label = test_pair[1]
  semantic_data = one_hot_encoding(semantic_label).float()
  test_data = test_data.to(device)
  semantic_data = semantic_data.to(device)
  semantic_label = semantic_label.to(device)

  with torch.no_grad():
    output = model(test_data)['out']
  output_predictions = output.argmax(1)
  output_predictions = output.argmax(1)
  output_predictions = output_predictions.reshape(-1,1)
  output_predictions = output_predictions.squeeze(1)
  output_predictions = output_predictions.to(device)
  semantic_label = semantic_label.reshape(-1,1)
  semantic_label = semantic_label.squeeze(1)
  mean = (semantic_label == output_predictions).sum()/len(output_predictions)
  return mean

def compute_class_accuracy(model,test_dataloader):
  avg_class_accuracy = []

def evaluate_class_iou(model1,test_dataloader,weight = 0.2):

  avg_model1_class_iou_list = []
  #avg_model2_class_iou_list = []
  #avg_ensemble_class_iou_list = []

  for i in tqdm(range(500)):
    test_pair = next(iter(test_dataloader))
    test_data = test_pair[0]
    semantic_label = test_pair[1]
    semantic_data = one_hot_encoding(semantic_label).float()
    test_data = test_data.to(device)
    semantic_data = semantic_data.to(device)
    semantic_label = semantic_label.to(device)
    count = i+1

    with torch.no_grad():
      model1_output = model1(test_data)['out']
      #model2_output = model2(test_data)['out']
      #ensemble_output = semantic_ensemble_eval(model1,model2,test_data,weight)
      model1_class_iou = class_IoU_compute(model1_output,semantic_label)
      #model2_class_iou = class_IoU_compute(model2_output,semantic_label)
      #ensemble_class_iou = class_IoU_compute(ensemble_output,semantic_label)
      if i == 1:
        avg_model1_class_iou_list = model1_class_iou
        #avg_model2_class_iou_list = model2_class_iou
        #avg_ensemble_class_iou_list = ensemble_class_iou
      else:
        avg_model1_class_iou_list += model1_class_iou
        #avg_model2_class_iou_list += model2_class_iou
        #avg_ensemble_class_iou_list += ensemble_class_iou  
    avg_model1_class_iou_list = avg_model1_class_iou_list
    #avg_model2_class_iou_list = avg_model2_class_iou_list
    #avg_ensemble_class_iou_list = avg_ensemble_class_iou_list     
  return avg_model1_class_iou_list

def evaluate_class_iou_ensemble(model1,model2,test_dataloader,weight = 0.2):

  #avg_model1_class_iou_list = []
  #avg_model2_class_iou_list = []
  avg_ensemble_class_iou_list = []

  for i in tqdm(range(500)):
    test_pair = next(iter(test_dataloader))
    test_data = test_pair[0]
    semantic_label = test_pair[1]
    semantic_data = one_hot_encoding(semantic_label).float()
    test_data = test_data.to(device)
    semantic_data = semantic_data.to(device)
    semantic_label = semantic_label.to(device)
    count = i+1

    with torch.no_grad():
      #model1_output = model1(test_data)['out']
      #model2_output = model2(test_data)['out']
      ensemble_output = semantic_ensemble_eval(model1,model2,test_data,weight)
      #model1_class_iou = class_IoU_compute(model1_output,semantic_label)
      #model2_class_iou = class_IoU_compute(model2_output,semantic_label)
      ensemble_class_iou = class_IoU_compute(ensemble_output,semantic_label)
      if i == 1:
        #avg_model1_class_iou_list = model1_class_iou
        #avg_model2_class_iou_list = model2_class_iou
        avg_ensemble_class_iou_list = ensemble_class_iou
      else:
        #avg_model1_class_iou_list += model1_class_iou
        #avg_model2_class_iou_list += model2_class_iou
        avg_ensemble_class_iou_list += ensemble_class_iou  
    #avg_model1_class_iou_list = avg_model1_class_iou_list
    #avg_model2_class_iou_list = avg_model2_class_iou_list
    avg_ensemble_class_iou_list = avg_ensemble_class_iou_list     
  return avg_ensemble_class_iou_list

"""new_FCN_model version 6"""

evaluate_model(new_FCN_model,loss_f,validation_dataloader)

evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,validation_dataloader)

"""Use new_FCN_model(version 6) and new_Deeplabv3 (version 10) """

evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,validation_dataloader)

"""weighted"""

evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,validation_dataloader,0.7)

new_FCN_model.to(device)
new_deeplabv3.to(device)
new_FCN_model.eval()
new_deeplabv3.eval()
evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,validation_dataloader,0.7)

visualize_performance_on_sample_image(new_FCN_model,"training",0)

visualize_performance_on_sample_image(new_deeplabv3,"training",0)

visualize_performance_on_sample_image_ensemble(new_FCN_model,new_deeplabv3,dataset = "training",index = 0,weight = 0.7)

new_FCN_model.to(device)
new_deeplabv3.to(device)
new_FCN_model.eval()
new_deeplabv3.eval()
evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,validation_dataloader,0.8)

weight_list = np.arange(0.1,1,0.05)
weight_list

#pick the best weight
mean_IoU_list = []
validation_loss_list = []
new_FCN_model.to(device)
new_deeplabv3.to(device)
for weight in weight_list:
  loss,iou = evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,validation_dataloader,weight)
  validation_loss_list.append(loss)
  mean_IoU_list.append(iou)
print("loss: ", validation_loss_list)
print("iou: ", mean_IoU_list)

from IPython.core.pylabtools import figsize
fig,ax = plt.subplots(1,2,figsize=(16,8))
ax[0].plot(weight_list,validation_loss_list,label = "validation loss")
ax[0].plot(weight_list,np.repeat(0.13913761889189483,len(weight_list)),label = "FCN validation loss baseline")
ax[0].plot(weight_list,np.repeat(0.948522412121296,len(weight_list)),label = "Deeplabv3 validation loss baseline")
ax[1].plot(weight_list,mean_IoU_list,label = "mean IoU")
ax[1].plot(weight_list,np.repeat(63.851236151348395,len(weight_list)),label = "FCN mean_IoU baseline")
ax[1].plot(weight_list,np.repeat(33.42754639826895,len(weight_list)),label = "Deeplabv3 mean_IoU baseline")
ax[1].legend()
ax[0].legend()
#plt.show()

weight_list_2 = np.arange(0.1,1,0.1)
mean_IoU_list = []
validation_loss_list = []
new_FCN_model.to(device)
new_deeplabv3.to(device)
new_FCN_model.eval()
new_deeplabv3.eval()
for weight in weight_list_2:
  loss,iou = evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,train_dataloader,weight)
  validation_loss_list.append(loss)
  mean_IoU_list.append(iou)
print("loss: ", validation_loss_list)
print("iou: ", mean_IoU_list)

from IPython.core.pylabtools import figsize
fig,ax = plt.subplots(1,2,figsize=(16,8))
ax[0].plot(weight_list,validation_loss_list,label = "validation loss")
ax[0].plot(weight_list,np.repeat(0.13913761889189483,len(weight_list)),label = "FCN validation loss baseline")
ax[0].plot(weight_list,np.repeat(0.948522412121296,len(weight_list)),label = "Deeplabv3 validation loss baseline")
ax[1].plot(weight_list,mean_IoU_list,label = "mean IoU")
ax[1].plot(weight_list,np.repeat(63.851236151348395,len(weight_list)),label = "FCN mean_IoU baseline")
ax[1].plot(weight_list,np.repeat(33.42754639826895,len(weight_list)),label = "Deeplabv3 mean_IoU baseline")
ax[1].legend()
ax[0].legend()

"""try model with low performance"""

new_FCN_model.to(device)
new_deeplabv3.to(device)
new_FCN_model.eval()
new_deeplabv3.eval()

evaluate_model(new_FCN_model,loss_f,train_dataloader)

evaluate_model(new_deeplabv3,loss_f,train_dataloader)

weight_list_2 = np.arange(0.1,1,0.1)
mean_IoU_list = []
validation_loss_list = []
new_FCN_model.to(device)
new_deeplabv3.to(device)
new_FCN_model.eval()
new_deeplabv3.eval()
for weight in weight_list_2:
  loss,iou = evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,train_dataloader,weight)
  validation_loss_list.append(loss)
  mean_IoU_list.append(iou)
print("loss: ", validation_loss_list)
print("iou: ", mean_IoU_list)

"""# Result for Report"""

model_name_9 = "new_FCN10"
path = '/content/drive/MyDrive/6.869/{model_name_9}'
new_FCN_model.load_state_dict(torch.load(path))

deeplabv3_10 = "new_deeplabv3_10"
path = '/content/drive/MyDrive/6.869/{deeplabv3_10}'
new_deeplabv3.load_state_dict(torch.load(path))

new_FCN_model.to(device)
new_deeplabv3.to(device)
new_FCN_model.eval()
new_deeplabv3.eval()

fcn_class_iou = evaluate_class_iou(new_FCN_model,train_dataloader)
print(fcn_class_iou)

deeplab_class_iou = evaluate_class_iou(new_deeplabv3,train_dataloader)
print(deeplab_class_iou)

ensemble_class_iou = evaluate_class_iou_ensemble(new_FCN_model,new_deeplabv3,train_dataloader,0.2)
print(ensemble_class_iou)

deeplab_list = np.asarray(deeplab_class_iou).reshape(499,150)
deeplab_list = np.nanmean(deeplab_list,axis=0)
fcn_list = np.asarray(fcn_class_iou).reshape(499,150)
fcn_list = np.nanmean(fcn_list,axis=0)
ensemble_list = np.asarray(ensemble_class_iou).reshape(499,150)
ensemble_list = np.nanmean(ensemble_list,axis=0)

temp1 = deeplab_class_iou[151:301]
temp2 = fcn_class_iou[151:301]
temp3 = ensemble_class_iou[151:301]
fig = plt.subplots(figsize =(16, 8))
plt.bar(np.arange(0,150), temp3,label = "ensemble")
plt.bar(np.arange(0,150), temp2,label = "fcn")
plt.bar(np.arange(0,150), temp1, label = "deeplab")
plt.legend()

fig = plt.subplots(figsize =(16, 8))
plt.bar(np.arange(0,150), ensemble_list*100,label = "ensemble average IoU")
plt.bar(np.arange(0,150), fcn_list*100,label = "fcn average IoU")
plt.bar(np.arange(0,150), deeplab_list*100, label = "deeplab average IoU")
plt.legend()

ensemble_list2 = ensemble_list[80:150]
fcn_list2 = fcn_list[80:150]
deeplab_list2 = deeplab_list[80:150]
fig = plt.subplots(figsize =(16, 8))
plt.bar(np.arange(80,150), ensemble_list2*100,label = "ensemble average IoU on less frequent class")
plt.bar(np.arange(80,150), fcn_list2*100,label = "fcn average IoU on less frequent class")
plt.bar(np.arange(80,150), deeplab_list2*100, label = "deeplab average IoU on less frequent class")
plt.legend()

ensemble_list3 = ensemble_list[0:80]
fcn_list3 = fcn_list[0:80]
deeplab_list3 = deeplab_list[0:80]
fig = plt.subplots(figsize =(16, 8))
plt.bar(np.arange(0,80), ensemble_list3*100,label = "ensemble average IoU on more frequent class")
plt.bar(np.arange(0,80), fcn_list3*100,label = "fcn average IoU on more frequent class")
plt.bar(np.arange(0,80), deeplab_list3*100, label = "deeplab average IoU on more frequent class")
plt.legend()

fig,ax = plt.subplots(1,2,figsize=(16,8))
ax[1].bar(np.arange(80,150), ensemble_list2*100,label = "ensemble average IoU on less frequent class")
ax[1].bar(np.arange(80,150), fcn_list2*100,label = "fcn average IoU on less frequent class")
ax[1].bar(np.arange(80,150), deeplab_list2*100, label = "deeplab average IoU on less frequent class")
ax[1].legend()
ax[0].bar(np.arange(0,80), ensemble_list3*100,label = "ensemble average IoU on more frequent class")
ax[0].bar(np.arange(0,80), fcn_list3*100,label = "fcn average IoU on more frequent class")
ax[0].bar(np.arange(0,80), deeplab_list3*100, label = "deeplab average IoU on more frequent class")
ax[0].legend()

fcn_avg_test_loss,fcn_avg_iou,fcn_avg_accuracy = evaluate_model(new_FCN_model,loss_f,train_dataloader)

deeplab_avg_test_loss,deeplab_avg_iou,deeplab_avg_accuracy = evaluate_model(new_deeplabv3,loss_f,train_dataloader)

weight_list = np.arange(0.1,1,0.1)
mean_IoU_list = []
mean_accuracy_list = []
validation_loss_list = []

new_FCN_model.to(device)
new_deeplabv3.to(device)
new_FCN_model.eval()
new_deeplabv3.eval()
for weight in weight_list:
  loss,iou,accuracy = evaluate_model_ensemble(new_FCN_model,new_deeplabv3,loss_f,train_dataloader,weight)
  validation_loss_list.append(loss)
  mean_IoU_list.append(iou)
  mean_accuracy_list.append(accuracy)

print("loss: ", validation_loss_list)
print("iou: ", mean_IoU_list)
print("accuracy: ", mean_accuracy_list)

weight_list = np.arange(0.1,1,0.1)

fcn_avg_iou = 28.158802637430945
fcn_avg_test_loss =   1.1717317619919776
fcn_avg_accuracy =   0.536683030217886

deeplab_avg_iou =   31.302984534269143
deeplab_avg_test_loss =   0.9340236086249352
deeplab_avg_accuracy =   0.5614824333786964

validation_loss_list = [0.9529526843130589, 0.9066683161556721, 0.9447183390855789, 0.9455011916458607, 0.9458680933713913, 0.94053090685606, 0.9850528735816478, 1.0300854046940804, 1.1005251532196998]
mean_IoU_list = [30.830429841231602, 31.559891837786832, 31.17958349274247, 30.85451865852983, 30.762040403982972, 30.789905306514317, 30.599294530688226, 30.15877443255289, 29.231268362084766]
mean_accuracy_list = [0.5598943469524383, 0.5775440048575401, 0.5655154955685139, 0.5753650854527951, 0.5693372937738895, 0.5670112200379371, 0.5616726441979408, 0.5643930578827858, 0.5476417412459851]

from IPython.core.pylabtools import figsize
fig,ax = plt.subplots(1,3,figsize=(16,8))
ax[0].plot(weight_list,validation_loss_list,label = "validation loss")
ax[0].plot(weight_list,np.repeat(fcn_avg_test_loss,len(weight_list)),label = "FCN validation loss baseline")
ax[0].plot(weight_list,np.repeat(deeplab_avg_test_loss,len(weight_list)),label = "Deeplabv3 validation loss baseline")

ax[1].plot(weight_list,mean_IoU_list,label = "mean IoU")
ax[1].plot(weight_list,np.repeat(fcn_avg_iou,len(weight_list)),label = "FCN mean_IoU baseline")
ax[1].plot(weight_list,np.repeat(deeplab_avg_iou,len(weight_list)),label = "Deeplabv3 mean_IoU baseline")

ax[2].plot(weight_list,mean_accuracy_list,label = "mean Accuracy")
ax[2].plot(weight_list,np.repeat(fcn_avg_accuracy,len(weight_list)),label = "FCN mean accuracy baseline")
ax[2].plot(weight_list,np.repeat(deeplab_avg_accuracy,len(weight_list)),label = "Deeplabv3 mean accuracy baseline")

ax[2].legend()
ax[1].legend()
ax[0].legend()

from IPython.core.pylabtools import figsize
fig,ax = plt.subplots(1,3,figsize=(12,6))
ax[0].plot(weight_list,validation_loss_list,label = "validation loss")
ax[0].plot(weight_list,np.repeat(fcn_avg_test_loss,len(weight_list)),label = "FCN validation loss baseline")
ax[0].plot(weight_list,np.repeat(deeplab_avg_test_loss,len(weight_list)),label = "Deeplabv3 validation loss baseline")

ax[1].plot(weight_list,mean_IoU_list,label = "mean IoU")
ax[1].plot(weight_list,np.repeat(fcn_avg_iou,len(weight_list)),label = "FCN mean_IoU baseline")
ax[1].plot(weight_list,np.repeat(deeplab_avg_iou,len(weight_list)),label = "Deeplabv3 mean_IoU baseline")

ax[2].plot(weight_list,mean_accuracy_list,label = "mean Accuracy")
ax[2].plot(weight_list,np.repeat(fcn_avg_accuracy,len(weight_list)),label = "FCN mean accuracy baseline")
ax[2].plot(weight_list,np.repeat(deeplab_avg_accuracy,len(weight_list)),label = "Deeplabv3 mean accuracy baseline")

ax[2].legend()
ax[1].legend()
ax[0].legend()

visualize_performance_on_sample_image(new_FCN_model,"training",1)

visualize_performance_on_sample_image(new_deeplabv3,"training",1)

visualize_performance_on_sample_image_ensemble(new_FCN_model,new_deeplabv3,"training",1)

image_name_list = ["ADE_train_00000009","ADE_train_00000007","ADE_train_00000008"]
fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)
for i,image_name in enumerate(image_name_list):
  original_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
  label_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR, "annotations", "training", f"{image_name}.png")
  original_img = Image.open(original_image).convert('RGB')
  mask_img = Image.open(label_image)

  axs[i,0].imshow(original_img)
  axs[i,0].grid(False)

  label_image = np.asarray(mask_img)
  axs[i,1].imshow(label_image)
  axs[i,1].grid(False)

image_name_list = ["ADE_train_00000002","ADE_train_00000003","ADE_train_00000005"]
fig, axs = plt.subplots(len(image_name_list), 2, figsize=(16, 8), constrained_layout=True)
for i,image_name in enumerate(image_name_list):
  original_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR,"images","training",f"{image_name}.jpg")
  label_image = os.path.join(TARGET_GOOGLE_DRIVE_DIR, "annotations", "training", f"{image_name}.png")
  original_img = Image.open(original_image).convert('RGB')
  mask_img = Image.open(label_image)

  axs[i,0].imshow(original_img)
  axs[i,0].grid(False)

  label_image = np.asarray(mask_img)
  axs[i,1].imshow(label_image)
  axs[i,1].grid(False)

visualize_performance_on_sample_image(new_FCN_model,"training",0)

visualize_performance_on_sample_image(new_deeplabv3,"training",0)

visualize_performance_on_sample_image_ensemble(new_FCN_model,new_deeplabv3,"training",0)

